{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d8a62cd-5edb-4457-b753-c98cfe3aaf1a",
   "metadata": {},
   "source": [
    "# ¿Cuál es tu mascota?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b056a4e1-44c3-4bfb-aeef-a0aa1ee247a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import os\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33a402b8-10a2-4f43-9ab4-0649143f3316",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = './dataset'\n",
    "train_dir = os.path.join(data_dir, 'training_set')\n",
    "test_dir = os.path.join(data_dir, 'test_set')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9618218-a67a-4d80-aa98-1d6683430823",
   "metadata": {},
   "source": [
    "#### Resumen General de las Transformaciones\n",
    "\n",
    "transforms.Compose:\n",
    "Combina múltiples transformaciones en una secuencia.\n",
    "Argumento: una lista de objetos de transformación.\n",
    "\n",
    "transforms.RandomResizedCrop (solo en entrenamiento):\n",
    "Recorta y redimensiona aleatoriamente la imagen.\n",
    "Parámetro: tamaño del recorte.\n",
    "\n",
    "transforms.RandomHorizontalFlip (solo en entrenamiento):\n",
    "Invierte la imagen horizontalmente con una probabilidad del 50%.\n",
    "\n",
    "transforms.Resize (solo en prueba):\n",
    "Redimensiona la imagen manteniendo la relación de aspecto.\n",
    "Parámetro: tamaño del lado más corto.\n",
    "\n",
    "\n",
    "transforms.CenterCrop (solo en prueba):\n",
    "Recorta el centro de la imagen.\n",
    "Parámetro: tamaño del recorte.\n",
    "Centro de la Imagen: CenterCrop intentará recortar una sección de 224x224 píxeles del centro de la imagen de 500x800. Esto funcionará si la imagen es suficientemente grande. Sin embargo, si la imagen es más pequeña que 224x224 píxeles en algún lado, este recorte no funcionará correctamente y puede resultar en errores o en imágenes recortadas inadecuadamente.\n",
    "\n",
    "transforms.ToTensor:\n",
    "Convierte la imagen en un tensor.\n",
    "\n",
    "transforms.Normalize:\n",
    "Normaliza los valores de los píxeles.\n",
    "Parámetros: listas de medias y desviaciones estándar para cada canal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "529cfb6a-05eb-454a-89dc-513a9d1538e9",
   "metadata": {},
   "source": [
    "#### Resumen de las Diferencias\n",
    "Entrenamiento:\n",
    "\n",
    "Objetivo: Aumentar la variabilidad y la robustez del modelo.\n",
    "Transformaciones: Incluyen aumentación de datos como RandomResizedCrop y RandomHorizontalFlip.\n",
    "Prueba:\n",
    "\n",
    "Objetivo: Evaluar el rendimiento del modelo de manera justa y consistente.\n",
    "Transformaciones: No incluyen aumentación de datos. Solo aseguran que las imágenes sean del tamaño adecuado y estén normalizadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6fb0b602-c5aa-4d63-90d6-80a0888c64bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trasnforms.Compose is an object\n",
    "# transforms.RandomResizedCrop() is an object\n",
    "train_transforms = transforms.Compose([   \n",
    "    transforms.RandomResizedCrop(224), # Crop a random region of an image and then resize to a fix size 224 is the size of the crop\n",
    "    transforms.RandomHorizontalFlip(), # Flips the image with a probability of 0.5\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Al asegurar que el lado más corto de todas las imágenes sea de 256 píxeles, nos garantizamos de que haya\n",
    "# suficiente área en la imagen para hacer un recorte central de 224x224 píxeles. \n",
    "test_transforms = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224), \n",
    "    transforms.ToTensor(), \n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])  # List of means and list of std (one per channel RGB)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b13e316a-5d57-4250-8a5a-eb4f72142ad1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset ImageFolder\n",
      "    Number of datapoints: 8000\n",
      "    Root location: ./dataset/training_set\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               RandomResizedCrop(size=(224, 224), scale=(0.08, 1.0), ratio=(0.75, 1.3333), interpolation=bilinear, antialias=True)\n",
      "               RandomHorizontalFlip(p=0.5)\n",
      "               ToTensor()\n",
      "               Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      "           )\n",
      "<class 'torchvision.datasets.folder.ImageFolder'>\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x7dafaeb70b80>\n",
      "<class 'torch.utils.data.dataloader.DataLoader'>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Importamos y aplicamos las transformaciones a cada imagen\n",
    "train_dataset = datasets.ImageFolder(root=train_dir, transform=train_transforms)\n",
    "test_dataset = datasets.ImageFolder(root=test_dir, transform=test_transforms)\n",
    "\n",
    "print(train_dataset)\n",
    "print(type(train_dataset))\n",
    "# Hacemos batches\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=4)\n",
    "\n",
    "print(train_loader)\n",
    "print(type(train_loader))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f315691-6cc5-4126-a307-a0eccc0b7614",
   "metadata": {},
   "source": [
    "## BUILD CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec23d349-2736-4c4e-8a19-735a7c049318",
   "metadata": {},
   "source": [
    "Primera Capa Convolucional:\n",
    "\n",
    "Entrada: 3 canales (RGB).\n",
    "Salida: 32 filtros.\n",
    "Tamaño del filtro: 3x3.\n",
    "Stride: 1.\n",
    "Segunda Capa Convolucional:\n",
    "\n",
    "Entrada: 32 filtros (de la primera capa).\n",
    "Salida: 64 filtros.\n",
    "Tamaño del filtro: 3x3.\n",
    "Stride: 1.\n",
    "Seguido por Max Pooling con tamaño de kernel 2x2.\n",
    "Tercera Capa Convolucional:\n",
    "\n",
    "Entrada: 64 filtros (de la segunda capa).\n",
    "Salida: 128 filtros.\n",
    "Tamaño del filtro: 3x3.\n",
    "Stride: 1.\n",
    "Seguido por Max Pooling con tamaño de kernel 2x2.\n",
    "Capas Completamente Conectadas:\n",
    "\n",
    "La salida de las capas convolucionales se aplana y pasa por una capa completamente conectada con 512 unidades.\n",
    "La capa final tiene 2 unidades para las 2 clases (perros y gatos)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cfc3b591-93e3-4307-8ded-a6e4b487d665",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, 3, 1)\n",
    "        self.fc1 = nn.Linear(128 * 54 * 54, 512)\n",
    "        self.fc2 = nn.Linear(512, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = F.max_pool2d(F.relu(self.conv3(x)), 2)\n",
    "        x = x.view(-1, 128 * 54 * 54)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = SimpleCNN().to(device)\n",
    "\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f9da73ea-f2f9-42dd-8e83-1251117deeec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/8000 (0%)]\tLoss: 0.694132\n",
      "Train Epoch: 0 [320/8000 (4%)]\tLoss: 0.706841\n",
      "Train Epoch: 0 [640/8000 (8%)]\tLoss: 0.685812\n",
      "Train Epoch: 0 [960/8000 (12%)]\tLoss: 0.697628\n",
      "Train Epoch: 0 [1280/8000 (16%)]\tLoss: 0.701136\n",
      "Train Epoch: 0 [1600/8000 (20%)]\tLoss: 0.691285\n",
      "Train Epoch: 0 [1920/8000 (24%)]\tLoss: 0.692559\n",
      "Train Epoch: 0 [2240/8000 (28%)]\tLoss: 0.695069\n",
      "Train Epoch: 0 [2560/8000 (32%)]\tLoss: 0.689085\n",
      "Train Epoch: 0 [2880/8000 (36%)]\tLoss: 0.689786\n",
      "Train Epoch: 0 [3200/8000 (40%)]\tLoss: 0.692772\n",
      "Train Epoch: 0 [3520/8000 (44%)]\tLoss: 0.686974\n",
      "Train Epoch: 0 [3840/8000 (48%)]\tLoss: 0.693434\n",
      "Train Epoch: 0 [4160/8000 (52%)]\tLoss: 0.678756\n",
      "Train Epoch: 0 [4480/8000 (56%)]\tLoss: 0.681976\n",
      "Train Epoch: 0 [4800/8000 (60%)]\tLoss: 0.705909\n",
      "Train Epoch: 0 [5120/8000 (64%)]\tLoss: 0.677231\n",
      "Train Epoch: 0 [5440/8000 (68%)]\tLoss: 0.698271\n",
      "Train Epoch: 0 [5760/8000 (72%)]\tLoss: 0.688481\n",
      "Train Epoch: 0 [6080/8000 (76%)]\tLoss: 0.688406\n",
      "Train Epoch: 0 [6400/8000 (80%)]\tLoss: 0.695982\n",
      "Train Epoch: 0 [6720/8000 (84%)]\tLoss: 0.692132\n",
      "Train Epoch: 0 [7040/8000 (88%)]\tLoss: 0.645124\n",
      "Train Epoch: 0 [7360/8000 (92%)]\tLoss: 0.675021\n",
      "Train Epoch: 0 [7680/8000 (96%)]\tLoss: 0.664825\n",
      "Train Epoch: 1 [0/8000 (0%)]\tLoss: 0.680116\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m output \u001b[38;5;241m=\u001b[39m model(data)\n\u001b[1;32m     11\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output, target)\n\u001b[0;32m---> 12\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_idx \u001b[38;5;241m%\u001b[39m log_interval \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/torch/_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    524\u001b[0m     )\n\u001b[0;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/torch/autograd/__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/torch/autograd/graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "log_interval = 10\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print(f'Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)} '\n",
    "                  f'({100. * batch_idx / len(train_loader):.0f}%)]\\tLoss: {loss.item():.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454f385f-e853-44e7-81b4-4857de52174c",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 5\n",
    "\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    train(model, device, train_loader, optimizer, criterion, epoch)\n",
    "    test(model, device, test_loader, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "865d89dc-408d-4956-b120-a0c6670bb09c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ea4518-27c7-40be-8d76-2c9348ebd6af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1daef715-494f-4f54-8209-f0d10a04ea1c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
