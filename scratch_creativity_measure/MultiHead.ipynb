{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b2e81b8-a81c-4e30-9f3b-2d197713e87c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x72cdf6c8df70>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import Counter\n",
    "import math\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "torch.manual_seed(23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bebb99b7-f58f-43a7-b68a-17f8624f3fec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA está disponible.\n",
      "Hay 1 GPU(s) disponible(s).\n",
      "GPU 0: NVIDIA GeForce RTX 2060\n"
     ]
    }
   ],
   "source": [
    "def check_gpu():\n",
    "    if torch.cuda.is_available():\n",
    "        print(\"CUDA está disponible.\")\n",
    "        print(f\"Hay {torch.cuda.device_count()} GPU(s) disponible(s).\")\n",
    "        for i in range(torch.cuda.device_count()):\n",
    "            print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "    else:\n",
    "        print(\"CUDA no está disponible. No hay GPU accesible.\")\n",
    "\n",
    "check_gpu()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "4e3dd2a0-8b70-49d2-93d5-0d7e9997c960",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model = 512, num_heads = 8):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0, 'Embedding size not compatible with num heads'\n",
    "        \n",
    "        self.d_v = d_model // num_heads\n",
    "        print(f\"Mi d_v {self.d_v}\")\n",
    "        self.d_k = self.d_v\n",
    "        self.num_heads = num_heads\n",
    "        \n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        print(f\"Mi W_q {self.W_q.weight.data}\")\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "        \n",
    "    def forward(self, Q, K, V, mask = None):\n",
    "        batch_size = Q.size(0)\n",
    "        '''\n",
    "        Q, K, V -> [batch_size, seq_len, num_heads*d_k]\n",
    "        after transpose Q, K, V -> [batch_size, num_heads, seq_len, d_k]\n",
    "        '''\n",
    "        print(\"-----PRUEBAS------\")\n",
    "        Q_test = self.W_q(Q)\n",
    "        print(\"Mi Q_test:\", Q_test)\n",
    "        print(\"Mi Q_test_dims:\", Q_test.shape)\n",
    "        print(\"Q_test.view():\", Q_test.view(batch_size, -1, self.num_heads, self.d_k))\n",
    "        print(\"Q_test.view().transpose():\", Q_test.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2 ))\n",
    "\n",
    "        \n",
    "        Q = self.W_q(Q).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2 )\n",
    "        K = self.W_k(K).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2 )\n",
    "        V = self.W_v(V).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2 )\n",
    "        \n",
    "        weighted_values, attention = self.scale_dot_product(Q, K, V, mask)\n",
    "        print(\"------------- PRUEBAS EN WEIGHTED_VALES -------------\")\n",
    "        weigh_val = weighted_values.transpose(1, 2)\n",
    "        print(\"Matrix Weight_val_transpose:\", weigh_val)\n",
    "        weigh_val_cont = weigh_val.contiguous()\n",
    "        print(\"Matrix Weight_val_transpose_contiguous:\", weigh_val)\n",
    "        weigh_val_views = weigh_val_cont.view(batch_size, -1, self.num_heads*self.d_k)\n",
    "        print(\"Matrix Weight_val_transpose_contiguous_views:\", weigh_val_views)\n",
    "        weighted_values = weighted_values.transpose(1, 2).contiguous().view(batch_size, -1, self.num_heads*self.d_k)\n",
    "        print(\"Matrix weights W_o:\", self.W_o.weight.data)\n",
    "        weighted_values = self.W_o(weighted_values)\n",
    "        print(\"Matrix weighted_values after W_o:\", weighted_values)\n",
    "        \n",
    "        return weighted_values, attention\n",
    "        \n",
    "        \n",
    "    def scale_dot_product(self, Q, K, V, mask = None):\n",
    "        print(\"-------- PRUEBAS EN SCALE_DOT_PRODUCT ------------\")\n",
    "        print(\"Mi matriz K original:\", K)\n",
    "        k_trans = K.transpose(-2, -1)\n",
    "        print(\"Mi matriz Q:\", Q)\n",
    "        print(\"Mi matriz K transposed\", k_trans)\n",
    "        qk = torch.matmul(Q, k_trans)\n",
    "        print(\"Matrix Q*K_trans:\", qk)\n",
    "\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        print(\"Scores:\", scores)\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "        attention = F.softmax(scores, dim = -1)\n",
    "        print(\"Attention:\", attention)\n",
    "        weighted_values = torch.matmul(attention, V)\n",
    "        print(\"Matrix V:\", V)\n",
    "        print(\"Weighted Values:\", weighted_values)\n",
    "        \n",
    "        return weighted_values, attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b9348406-c0b9-4beb-8119-38f6d02a89bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: tensor([[[0.4173, 0.5896, 0.0422, 0.3887],\n",
      "         [0.9875, 0.7190, 0.3843, 0.0279]]])\n",
      "Q shape: torch.Size([1, 2, 4])\n",
      "Mi d_v 2\n",
      "Mi W_q tensor([[-0.3608,  0.1253,  0.0319,  0.4562],\n",
      "        [ 0.1704, -0.4905, -0.3813, -0.3382],\n",
      "        [-0.0396, -0.4052, -0.0390, -0.4643],\n",
      "        [ 0.1030,  0.4129, -0.3891, -0.2233]])\n",
      "-----PRUEBAS------\n",
      "Mi Q_test: tensor([[[-0.3142, -0.4374, -0.8772, -0.2649],\n",
      "         [-0.6574, -0.4121, -0.7980, -0.2054]]], grad_fn=<ViewBackward0>)\n",
      "Mi Q_test_dims: torch.Size([1, 2, 4])\n",
      "Q_test.view(): tensor([[[[-0.3142, -0.4374],\n",
      "          [-0.8772, -0.2649]],\n",
      "\n",
      "         [[-0.6574, -0.4121],\n",
      "          [-0.7980, -0.2054]]]], grad_fn=<ViewBackward0>)\n",
      "Q_test.view().transpose(): tensor([[[[-0.3142, -0.4374],\n",
      "          [-0.6574, -0.4121]],\n",
      "\n",
      "         [[-0.8772, -0.2649],\n",
      "          [-0.7980, -0.2054]]]], grad_fn=<TransposeBackward0>)\n",
      "-------- PRUEBAS EN SCALE_DOT_PRODUCT ------------\n",
      "Mi matriz K original: tensor([[[[-0.0566,  0.1305],\n",
      "          [ 0.0128,  0.0497]],\n",
      "\n",
      "         [[ 0.7868,  0.3014],\n",
      "          [ 0.9345,  0.3854]]]], grad_fn=<TransposeBackward0>)\n",
      "Mi matriz Q: tensor([[[[-0.3142, -0.4374],\n",
      "          [-0.6574, -0.4121]],\n",
      "\n",
      "         [[-0.8772, -0.2649],\n",
      "          [-0.7980, -0.2054]]]], grad_fn=<TransposeBackward0>)\n",
      "Mi matriz K transposed tensor([[[[-0.0566,  0.0128],\n",
      "          [ 0.1305,  0.0497]],\n",
      "\n",
      "         [[ 0.7868,  0.9345],\n",
      "          [ 0.3014,  0.3854]]]], grad_fn=<TransposeBackward0>)\n",
      "Matrix Q*K_trans: tensor([[[[-0.0393, -0.0258],\n",
      "          [-0.0165, -0.0289]],\n",
      "\n",
      "         [[-0.7700, -0.9218],\n",
      "          [-0.6897, -0.8248]]]], grad_fn=<UnsafeViewBackward0>)\n",
      "Scores: tensor([[[[-0.0278, -0.0182],\n",
      "          [-0.0117, -0.0205]],\n",
      "\n",
      "         [[-0.5445, -0.6518],\n",
      "          [-0.4877, -0.5832]]]], grad_fn=<DivBackward0>)\n",
      "Attention: tensor([[[[0.4976, 0.5024],\n",
      "          [0.5022, 0.4978]],\n",
      "\n",
      "         [[0.5268, 0.4732],\n",
      "          [0.5239, 0.4761]]]], grad_fn=<SoftmaxBackward0>)\n",
      "Matrix V: tensor([[[[ 0.3365,  0.0843],\n",
      "          [ 0.8859,  0.6239]],\n",
      "\n",
      "         [[-0.3650, -0.2520],\n",
      "          [-0.9120, -0.1866]]]], grad_fn=<TransposeBackward0>)\n",
      "Weighted Values: tensor([[[[ 0.6125,  0.3554],\n",
      "          [ 0.6100,  0.3529]],\n",
      "\n",
      "         [[-0.6238, -0.2211],\n",
      "          [-0.6254, -0.2209]]]], grad_fn=<UnsafeViewBackward0>)\n",
      "------------- PRUEBAS EN WEIGHTED_VALES -------------\n",
      "Matrix Weight_val_transpose: tensor([[[[ 0.6125,  0.3554],\n",
      "          [-0.6238, -0.2211]],\n",
      "\n",
      "         [[ 0.6100,  0.3529],\n",
      "          [-0.6254, -0.2209]]]], grad_fn=<TransposeBackward0>)\n",
      "Matrix Weight_val_transpose_contiguous: tensor([[[[ 0.6125,  0.3554],\n",
      "          [-0.6238, -0.2211]],\n",
      "\n",
      "         [[ 0.6100,  0.3529],\n",
      "          [-0.6254, -0.2209]]]], grad_fn=<TransposeBackward0>)\n",
      "Matrix Weight_val_transpose_contiguous_views: tensor([[[ 0.6125,  0.3554, -0.6238, -0.2211],\n",
      "         [ 0.6100,  0.3529, -0.6254, -0.2209]]], grad_fn=<ViewBackward0>)\n",
      "Matrix weights W_o: tensor([[-0.2936,  0.3505,  0.2247,  0.1517],\n",
      "        [ 0.2601, -0.4185,  0.1791,  0.0213],\n",
      "        [ 0.4687,  0.2881,  0.2865, -0.4198],\n",
      "        [-0.1541,  0.0847,  0.1682, -0.2017]])\n",
      "Matrix weighted_values after W_o: tensor([[[-0.0594, -0.0928,  0.5748, -0.4966],\n",
      "         [-0.0599, -0.0927,  0.5724, -0.4967]]], grad_fn=<ViewBackward0>)\n",
      "Output shape: torch.Size([1, 2, 4])\n",
      "Attention shape: torch.Size([1, 2, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "# Testeo de la clase\n",
    "d_model = 4 # Original: 512\n",
    "num_heads = 2 # Original: 8\n",
    "seq_len = 2 # Original: 10\n",
    "batch_size = 1 # Original: 32\n",
    "\n",
    "# Crear tensores Q, K, V aleatorios\n",
    "Q = torch.rand(batch_size, seq_len, d_model)\n",
    "print(f\"Q: {Q}\")\n",
    "print(f\"Q shape: {Q.shape}\")\n",
    "K = torch.rand(batch_size, seq_len, d_model)\n",
    "V = torch.rand(batch_size, seq_len, d_model)\n",
    "\n",
    "# Crear una instancia de MultiHeadAttention\n",
    "mha = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "# Llamar al método forward para testeo\n",
    "output, attention = mha(Q, K, V)\n",
    "\n",
    "print(\"Output shape:\", output.shape)   # Debe ser [batch_size, seq_len, d_model]\n",
    "print(\"Attention shape:\", attention.shape) # Debe ser [batch_size, num_heads, seq_len, seq_len]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95751c81-8d1b-45fe-ad4f-dd4eed3f5aa4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
