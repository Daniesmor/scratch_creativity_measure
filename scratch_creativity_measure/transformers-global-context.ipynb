{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01fc6c07-93fb-445d-98bb-511d33e6584e",
   "metadata": {},
   "source": [
    "## LOCAL AND GLOBAL EMBEDDINGS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e5d854-6b6f-42d8-ba5e-95e25bb1305b",
   "metadata": {},
   "source": [
    "This file its copy of the file **Transformers** but uses local and global context for each block of the scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "100719cd-ae66-444f-96fb-e53605c7aa4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x767bf80d0f90>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import Counter\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "\n",
    "torch.manual_seed(23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9857a5a4-c97d-45b0-a234-c8cbbfb84eae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA está disponible.\n",
      "Hay 1 GPU(s) disponible(s).\n",
      "GPU 0: NVIDIA GeForce RTX 2060\n"
     ]
    }
   ],
   "source": [
    "def check_gpu():\n",
    "    if torch.cuda.is_available():\n",
    "        print(\"CUDA está disponible.\")\n",
    "        print(f\"Hay {torch.cuda.device_count()} GPU(s) disponible(s).\")\n",
    "        for i in range(torch.cuda.device_count()):\n",
    "            print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "    else:\n",
    "        print(\"CUDA no está disponible. No hay GPU accesible.\")\n",
    "\n",
    "check_gpu()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#device = torch.device('cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dae9c193-ed4a-46c4-a909-f43d6528e55e",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQ_LEN = 128 # max num of words per phrase for translate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "11c3a800-7a10-40af-aae1-a013bc6346e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(nn.Module):\n",
    "    def __init__(self, d_model, max_seq_len = MAX_SEQ_LEN):\n",
    "        super().__init__()\n",
    "        self.pos_embed_matrix = torch.zeros(max_seq_len, d_model, device=device)\n",
    "        token_pos = torch.arange(0, max_seq_len, dtype = torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() \n",
    "                             * (-math.log(10000.0)/d_model))\n",
    "        self.pos_embed_matrix[:, 0::2] = torch.sin(token_pos * div_term)\n",
    "        self.pos_embed_matrix[:, 1::2] = torch.cos(token_pos * div_term)\n",
    "        self.pos_embed_matrix = self.pos_embed_matrix.unsqueeze(0).transpose(0,1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "#         print(self.pos_embed_matrix.shape)\n",
    "#         print(x.shape)\n",
    "        return x + self.pos_embed_matrix[:x.size(0), :]\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model = 512, num_heads = 8):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0, 'Embedding size not compatible with num heads'\n",
    "        \n",
    "        self.d_v = d_model // num_heads\n",
    "        self.d_k = self.d_v\n",
    "        self.num_heads = num_heads\n",
    "        \n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "        \n",
    "    def forward(self, Q, K, V, mask = None):\n",
    "        batch_size = Q.size(0)\n",
    "        '''\n",
    "        Q, K, V -> [batch_size, seq_len, num_heads*d_k]\n",
    "        after transpose Q, K, V -> [batch_size, num_heads, seq_len, d_k]\n",
    "        '''\n",
    "        Q = self.W_q(Q).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2 )\n",
    "        K = self.W_k(K).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2 )\n",
    "        V = self.W_v(V).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2 )\n",
    "        \n",
    "        weighted_values, attention = self.scale_dot_product(Q, K, V, mask)\n",
    "        weighted_values = weighted_values.transpose(1, 2).contiguous().view(batch_size, -1, self.num_heads*self.d_k)\n",
    "        weighted_values = self.W_o(weighted_values)\n",
    "        \n",
    "        return weighted_values, attention\n",
    "        \n",
    "        \n",
    "    def scale_dot_product(self, Q, K, V, mask = None):\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "        attention = F.softmax(scores, dim = -1)\n",
    "        weighted_values = torch.matmul(attention, V)\n",
    "        \n",
    "        return weighted_values, attention\n",
    "        \n",
    "\n",
    "class PositionFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.linear2(F.relu(self.linear1(x)))\n",
    "    \n",
    "class EncoderSubLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout = 0.1):\n",
    "        super().__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ffn = PositionFeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.droupout1 = nn.Dropout(dropout)\n",
    "        self.droupout2 = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x, mask = None):\n",
    "        attention_score, _ = self.self_attn(x, x, x, mask)\n",
    "        x = x + self.droupout1(attention_score)\n",
    "        x = self.norm1(x)\n",
    "        x = x + self.droupout2(self.ffn(x))\n",
    "        return self.norm2(x)\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, num_layers, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([EncoderSubLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "    def forward(self, x, mask=None):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return self.norm(x)\n",
    "\n",
    "class DecoderSubLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.cross_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feed_forward = PositionFeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.dropout3 = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, encoder_output, target_mask=None, encoder_mask=None):\n",
    "        attention_score, _ = self.self_attn(x, x, x, target_mask)\n",
    "        x = x + self.dropout1(attention_score)\n",
    "        x = self.norm1(x)\n",
    "        \n",
    "        encoder_attn, _ = self.cross_attn(x, encoder_output, encoder_output, encoder_mask)\n",
    "        x = x + self.dropout2(encoder_attn)\n",
    "        x = self.norm2(x)\n",
    "        \n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = x + self.dropout3(ff_output)\n",
    "        return self.norm3(x)\n",
    "        \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, num_layers, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([DecoderSubLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        \n",
    "    def forward(self, x, encoder_output, target_mask, encoder_mask):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, encoder_output, target_mask, encoder_mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c19e00c-3131-47a7-b2b1-3f3d80460b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, num_layers,\n",
    "                 input_vocab_size, max_len=MAX_SEQ_LEN, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.encoder_embedding = nn.Embedding(input_vocab_size, d_model)\n",
    "        self.pos_embedding = PositionalEmbedding(d_model, max_len)\n",
    "        self.encoder = Encoder(d_model, num_heads, d_ff, num_layers, dropout)\n",
    "        self.sep_token_id = 2\n",
    "\n",
    "        # Embeddings\n",
    "        self._cached_source_embeddings = None\n",
    "        self._all_embeddings = None\n",
    "        \n",
    "    def forward(self, source):\n",
    "        # Encoder mask\n",
    "        source_mask = self.mask(source)\n",
    "        # Embedding and positional Encoding\n",
    "        source = self.encoder_embedding(source) * math.sqrt(self.encoder_embedding.embedding_dim)\n",
    "        self._cached_source_embeddings = source\n",
    "        source = self.pos_embedding(source)\n",
    "        # Encoder\n",
    "        encoder_output = self.encoder(source, source_mask)\n",
    "        \n",
    "        return encoder_output\n",
    "        \n",
    "    def get_embeddings(self):\n",
    "        if self._all_embeddings is None:\n",
    "            raise ValueError(\"Embeddings not computed yet. Call forward() first.\")\n",
    "        return self._all_embeddings\n",
    "    \n",
    "    def mask(self, source):\n",
    "        source_mask = (source != 0).unsqueeze(1).unsqueeze(2)\n",
    "        \n",
    "        if self.sep_token_id is not None:\n",
    "            seq_len = source.size(1)\n",
    "            sep_positions = (source == self.sep_token_id).nonzero(as_tuple=True)[1]\n",
    "            \n",
    "            attention_block_mask = torch.ones(seq_len, seq_len, dtype=torch.bool, device=source.device)\n",
    "\n",
    "            last_sep_pos = -1\n",
    "            for sep_pos in sep_positions:\n",
    "                attention_block_mask[last_sep_pos + 1 : sep_pos, last_sep_pos + 1 : sep_pos] = 0\n",
    "                last_sep_pos = sep_pos\n",
    "\n",
    "            source_mask = source_mask & attention_block_mask.unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "        return source_mask\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e1020ca-4a1d-46ba-9eed-eb832cfd8948",
   "metadata": {},
   "source": [
    "## Simple test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "57412d7c-638a-4e5e-a4b9-9265f2f7f424",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output.shape torch.Size([2, 10, 512])\n",
      "tensor([[39, 11,  6, 31, 11, 42, 49, 35, 10, 20],\n",
      "        [24, 12, 35, 20, 30,  6, 23, 39, 48,  5]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "seq_len_source = 10\n",
    "seq_len_target = 10\n",
    "batch_size = 2\n",
    "input_vocab_size = 50\n",
    "target_vocab_size = 50\n",
    "\n",
    "source = torch.randint(1, input_vocab_size, (batch_size, seq_len_source))\n",
    "\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "d_ff = 2048\n",
    "num_layers = 6\n",
    "\n",
    "model = Transformer(d_model, num_heads, d_ff, num_layers,\n",
    "                   input_vocab_size, max_len=MAX_SEQ_LEN, dropout=0.1)\n",
    "\n",
    "\n",
    "model = model.to(device)\n",
    "source = source.to(device)\n",
    "\n",
    "output = model(source)\n",
    "#Expected output shape -> [batch, seq_len_target, target_vocab_size] i.e [2, 10, 50]\n",
    "print(f'output.shape {output.shape}')\n",
    "print(source)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2772bf1-598e-48f1-b172-7a98f6743b14",
   "metadata": {},
   "source": [
    "## DATA PREPROCESSING: Creation of scripts list with all project scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b412dbbf-8b6b-4fdb-83b5-161da29f212a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from zipfile import ZipFile, BadZipFile\n",
    "import json\n",
    "\n",
    "metrics = pd.read_csv('metrics_attr.csv')\n",
    "\n",
    "# Filter by Action Genre\n",
    "metrics_action = metrics[(metrics['Main Genre'] == 'Action')]\n",
    "metrics_non_action = metrics[(metrics['Main Genre'] != 'Action')]\n",
    "\n",
    "\n",
    "# Check the filenames for collect\n",
    "filenames_action = list(metrics_action['Name'])\n",
    "filenames_non_action = list(metrics_non_action['Name'])\n",
    "# Create txt for made the shell script\n",
    "with open('filenames_action_global.txt', 'w') as names:\n",
    "     for name in filenames_action:\n",
    "        names.write(name + '\\n')\n",
    "\n",
    "\n",
    "#for project in filenames_action:\n",
    " #   sb3_path = f'./projects_sb3/{project}'\n",
    "  #  if os.path.isfile(sb3_path):\n",
    "   #     shutil.copy(sb3_path, './sb3_action_global')\n",
    "    #    print(f'The project {project} has been success copy')\n",
    "    #else:\n",
    "     #   print(f'The project {project} doesnt exists')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "361746f1-83d6-48de-97be-008c2e3bd26c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "312\n",
      "312\n",
      "Abby and Grace's project.sb3\n",
      "312\n",
      "328\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# ----------------- SCRIPTS GLOBAL (local context) ----------------------------------\n",
    "dict_total_blocks = {}\n",
    "scripts_global = []\n",
    "list_total_blocks = []\n",
    "print(len(filenames_action))\n",
    "for project in filenames_action:\n",
    "    sb3_path = os.path.join('.','sb3_action_global',project)\n",
    "    if os.path.isfile(sb3_path):\n",
    "        #print(project)\n",
    "        json_project = load_json_project(sb3_path)\n",
    "        dict_total_blocks = process(json_project)\n",
    "\n",
    "        for sprite, seqs in dict_total_blocks.items():\n",
    "            for idx, block_list in seqs.items():\n",
    "                if block_list != []:\n",
    "                    scripts_global.append(\" \".join(block_list))\n",
    "\n",
    "# ----------------- SCRIPTS TARGET ----------------------------------\n",
    "\n",
    "scripts_train1 = []\n",
    "scripts_train2 = []\n",
    "for idx, project in enumerate(filenames_action):\n",
    "    sb3_path = os.path.join('.','sb3_action_global',project)\n",
    "    if os.path.isfile(sb3_path):\n",
    "        #print(project)\n",
    "        json_project = load_json_project(sb3_path)\n",
    "        dict_total_blocks = process(json_project)\n",
    "\n",
    "        for sprite, seqs in dict_total_blocks.items():\n",
    "            for block_list in seqs.values():\n",
    "                if block_list != []:\n",
    "                    if idx < math.floor(len(filenames_action)/2):\n",
    "                        scripts_train1.append(\" \".join(block_list))\n",
    "                    else:\n",
    "                        scripts_train2.append(\" \".join(block_list))\n",
    "\n",
    "\n",
    "# ----------------- SCRIPTS NEGTATIVE  ----------------------------------\n",
    "scripts_train3 = []\n",
    "for project in filenames_non_action:\n",
    "    sb3_path = os.path.join('.','sb3_non_action',project)\n",
    "    if os.path.isfile(sb3_path):\n",
    "        #print(project)\n",
    "        json_project = load_json_project(sb3_path)\n",
    "        dict_total_blocks = process(json_project)\n",
    "\n",
    "        for sprite, seqs in dict_total_blocks.items():\n",
    "            for idx, block_list in seqs.items():\n",
    "                if block_list != []:\n",
    "                    scripts_train3.append(\" \".join(block_list))\n",
    "\n",
    "# ----------------- SCRIPTS GLOBAL (global context)  ----------------------------------\n",
    "\n",
    "dict_total_blocks = {}\n",
    "scripts_global_global = []\n",
    "list_total_blocks = []\n",
    "print(len(filenames_action))\n",
    "print(filenames_action[0])\n",
    "for project in filenames_action:\n",
    "    \n",
    "    sb3_path = os.path.join('.','sb3_action_global',project)\n",
    "    if os.path.isfile(sb3_path):\n",
    "        #print(project)\n",
    "        json_project = load_json_project(sb3_path)\n",
    "        dict_total_blocks = process(json_project)\n",
    "\n",
    "        for sprite, seqs in dict_total_blocks.items():\n",
    "            global_seqs = []\n",
    "            #print(\"project:\", project)\n",
    "            #print(seqs)\n",
    "            #print(\"Sprite\",sprite)\n",
    "            \n",
    "            for idx, block_list in seqs.items():\n",
    "                if block_list != []:\n",
    "                    global_seqs.append(\" \".join(block_list))\n",
    "                    global_seqs.append(\"<SCRIPT_END>\")\n",
    "            if global_seqs != []:\n",
    "                global_seqs.pop(-1)\n",
    "            \n",
    "            scripts_global_global.append(\" \".join(global_seqs))\n",
    "\n",
    "\n",
    "# ----------------- SCRIPTS TARGET (POSITIV) (global context)  ----------------------------------\n",
    "\n",
    "scripts_train1_global = []\n",
    "scripts_train2_global = []\n",
    "dict_total_blocks = {}\n",
    "list_total_blocks = []\n",
    "print(len(filenames_action))\n",
    "for idx, project in enumerate(filenames_action):\n",
    "    \n",
    "    sb3_path = os.path.join('.','sb3_action_global',project)\n",
    "    if os.path.isfile(sb3_path):\n",
    "        #print(project)\n",
    "        json_project = load_json_project(sb3_path)\n",
    "        dict_total_blocks = process(json_project)\n",
    "\n",
    "        for sprite, seqs in dict_total_blocks.items():\n",
    "            global_seqs = []\n",
    "            #print(\"project:\", project)\n",
    "            #print(seqs)\n",
    "            #print(\"Sprite\",sprite)\n",
    "            \n",
    "            for block_list in seqs.values():\n",
    "                if block_list != []:  \n",
    "                    global_seqs.append(\" \".join(block_list))\n",
    "                    global_seqs.append(\"<SCRIPT_END>\")\n",
    "            if global_seqs != []:\n",
    "                global_seqs.pop(-1)\n",
    "        \n",
    "            if int(idx) < math.floor(len(filenames_action)/2):\n",
    "                scripts_train1_global.append(\" \".join(global_seqs))\n",
    "            else:\n",
    "                scripts_train2_global.append(\" \".join(global_seqs))\n",
    "\n",
    "# ----------------- SCRIPTS NEGATIVE (global context) -----------------------------\n",
    "\n",
    "dict_total_blocks = {}\n",
    "scripts_train3_global= []\n",
    "list_total_blocks = []\n",
    "print(len(filenames_non_action))\n",
    "for project in filenames_non_action:\n",
    "    \n",
    "    sb3_path = os.path.join('.','sb3_non_action',project)\n",
    "    if os.path.isfile(sb3_path):\n",
    "        #print(project)\n",
    "        json_project = load_json_project(sb3_path)\n",
    "        dict_total_blocks = process(json_project)\n",
    "\n",
    "        for sprite, seqs in dict_total_blocks.items():\n",
    "            global_seqs = []\n",
    "            #print(\"project:\", project)\n",
    "            #print(seqs)\n",
    "            #print(\"Sprite\",sprite)\n",
    "            \n",
    "            for idx, block_list in seqs.items():\n",
    "                if block_list != []:\n",
    "                    global_seqs.append(\" \".join(block_list))\n",
    "                    global_seqs.append(\"<SCRIPT_END>\")\n",
    "            if global_seqs != []:\n",
    "                global_seqs.pop(-1)\n",
    "            \n",
    "            scripts_train3_global.append(\" \".join(global_seqs))\n",
    "\n",
    "\n",
    "# universal_scripts -> scripts of all genres for local context\n",
    "# scripts_global -> global scripts (local context)... each index of the list its an scripts ACTION\n",
    "# scripts_train1 -> src\n",
    "# scritps_train2 -> trg\n",
    "# scritps_train3 -> negative\n",
    "# scripts_global_global -> global scripts (global context)... each index of the list its all project scripts \n",
    "\n",
    "# --------------------- LOCAL CONTEXT ---------------------------------------\n",
    "min_len = min(len(scripts_train1), len(scripts_train2), len(scripts_train3))\n",
    "scripts_train1 = scripts_train1[:min_len]\n",
    "scripts_train2 = scripts_train2[:min_len]\n",
    "scripts_train3 = scripts_train3[:min_len]\n",
    "\n",
    "scripts_universal = scripts_train1 + scripts_train2 + scripts_train3\n",
    "scripts_global = ['<sos> ' + script + ' <eos>' for script in scripts_global]\n",
    "scripts_train1 = ['<sos> ' + script + ' <eos>' for script in scripts_train1]\n",
    "scripts_train2 = ['<sos> ' + script + ' <eos>' for script in scripts_train2]\n",
    "scripts_train3 = ['<sos> ' + script + ' <eos>' for script in scripts_train3]\n",
    "\n",
    "# --------------------- GLOBAL CONTEXT ---------------------------------------\n",
    "min_len = min(len(scripts_train1_global), len(scripts_train2_global), len(scripts_train3_global))\n",
    "scripts_train1_global = scripts_train1_global[:min_len]\n",
    "scripts_train2_global = scripts_train2_global[:min_len]\n",
    "scripts_train3_global = scripts_train3_global[:min_len]\n",
    "\n",
    "scripts_universal_global = scripts_train1_global + scripts_train2_global + scripts_train3_global\n",
    "scripts_global_global = ['<sos> ' + script + ' <eos>' for script in scripts_global_global]\n",
    "scripts_train1_global = ['<sos> ' + script + ' <eos>' for script in scripts_train1_global]\n",
    "scripts_train2_global = ['<sos> ' + script + ' <eos>' for script in scripts_train2_global]\n",
    "scripts_train3_global = ['<sos> ' + script + ' <eos>' for script in scripts_train3_global]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d52017ad-9591-4c0f-989f-f381c12b8502",
   "metadata": {},
   "source": [
    "### CREATION OF DATASET FOR **TRAIN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1155d408-421c-4ff7-9862-5f8bf5212c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(scripts):\n",
    "    blocks = [block for script in scripts for block in script.split() ]\n",
    "    block_count = Counter(blocks)\n",
    "    sorted_block_counts = sorted(block_count.items(), key=lambda x:x[1], reverse=True)\n",
    "    block2idx = {\n",
    "        '<pad>': 0,\n",
    "        '<unk>': 1,\n",
    "        '<SCRIPT_END>': 2\n",
    "    }\n",
    "    for idx, (block, _) in enumerate(sorted_block_counts, 3):\n",
    "        block2idx[block] = idx\n",
    "    idx2block = {idx: block for block, idx in block2idx.items()}\n",
    "    return block2idx, idx2block\n",
    "\n",
    "class TripletDataset(Dataset):\n",
    "    def __init__(self, src_sentences, trg_sentences, neg_sentences, uni_block2idx):\n",
    "        self.src_sentences = src_sentences  # Anchor (source)\n",
    "        self.trg_sentences = trg_sentences  # Positive (target)\n",
    "        self.neg_sentences = neg_sentences  # Negative\n",
    "        self.src_block2idx = uni_block2idx\n",
    "        self.trg_block2idx = uni_block2idx\n",
    "        self.neg_block2idx = uni_block2idx\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.trg_sentences)  # Usamos la longitud del conjunto positivo\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Obtener las oraciones (anchor, positive, negative)\n",
    "        src_sentence = self.src_sentences[idx]\n",
    "        trg_sentence = self.trg_sentences[idx]\n",
    "        neg_sentence = self.neg_sentences[idx]\n",
    "\n",
    "        # Convertir cada oración en índices\n",
    "        src_idxs = [self.src_block2idx.get(block, self.src_block2idx['<unk>']) for block in src_sentence.split()]\n",
    "        trg_idxs = [self.trg_block2idx.get(block, self.trg_block2idx['<unk>']) for block in trg_sentence.split()]\n",
    "        neg_idxs = [self.neg_block2idx.get(block, self.neg_block2idx['<unk>']) for block in neg_sentence.split()]\n",
    "\n",
    "        # Retornar los tensores (anchor, positive, negative)\n",
    "        return torch.tensor(src_idxs), torch.tensor(trg_idxs), torch.tensor(neg_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "83ed041e-0530-4655-ab22-e4ac3c935fce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "142\n",
      "142\n",
      "{'<pad>': 0, '<unk>': 1, '<SCRIPT_END>': 2, '<sos>': 3, '<eos>': 4, 'looks_hide': 5, 'event_whenflagclicked': 6, 'event_whenbackdropswitchesto': 7, 'control_if': 8, 'looks_show': 9, 'control_wait': 10, 'control_forever': 11, 'data_changevariableby': 12, 'motion_gotoxy': 13, 'sensing_touchingobject': 14, 'sensing_touchingobjectmenu': 15, 'event_whenbroadcastreceived': 16, 'operator_equals': 17, 'data_setvariableto': 18, 'looks_switchcostumeto': 19, 'looks_costume': 20, 'looks_backdrops': 21, 'looks_switchbackdropto': 22, 'looks_sayforsecs': 23, 'event_whenkeypressed': 24, 'event_broadcast': 25, 'motion_glidesecstoxy': 26, 'event_whenthisspriteclicked': 27, 'operator_random': 28, 'sound_sounds_menu': 29, 'sound_play': 30, 'sensing_keypressed': 31, 'sensing_keyoptions': 32, 'control_stop': 33, 'control_wait_until': 34, 'operator_and': 35, 'data_hidevariable': 36, 'operator_gt': 37, 'sensing_touchingcolor': 38, 'operator_lt': 39, 'motion_movesteps': 40, 'motion_changeyby': 41, 'control_repeat': 42, 'control_if_else': 43, 'looks_backdropnumbername': 44, 'motion_goto': 45, 'motion_goto_menu': 46, 'looks_setsizeto': 47, 'motion_pointindirection': 48, 'motion_changexby': 49, 'operator_or': 50, 'control_repeat_until': 51, 'operator_not': 52, 'motion_sety': 53, 'music_playNoteForBeats': 54, 'note': 55, 'motion_ifonedgebounce': 56, 'data_showvariable': 57, 'procedures_call': 58, 'control_create_clone_of': 59, 'control_create_clone_of_menu': 60, 'sensing_mousedown': 61, 'control_start_as_clone': 62, 'looks_nextcostume': 63, 'looks_say': 64, 'looks_changeeffectby': 65, 'motion_turnright': 66, 'motion_yposition': 67, 'control_delete_this_clone': 68, 'sensing_answer': 69, 'operator_multiply': 70, 'looks_costumenumbername': 71, 'procedures_definition': 72, 'procedures_prototype': 73, 'sensing_of': 74, 'sensing_of_object_menu': 75, 'looks_gotofrontback': 76, 'motion_setx': 77, 'looks_changesizeby': 78, 'operator_add': 79, 'data_listcontainsitem': 80, 'data_addtolist': 81, 'sound_playuntildone': 82, 'looks_goforwardbackwardlayers': 83, 'operator_subtract': 84, 'sensing_coloristouchingcolor': 85, 'motion_direction': 86, 'sound_stopallsounds': 87, 'sensing_askandwait': 88, 'motion_xposition': 89, 'motion_turnleft': 90, 'argument_reporter_string_number': 91, 'sensing_distanceto': 92, 'sensing_distancetomenu': 93, 'sound_setvolumeto': 94, 'looks_thinkforsecs': 95, 'looks_switchbackdroptoandwait': 96, 'data_itemoflist': 97, 'operator_divide': 98, 'looks_seteffectto': 99, 'pen_clear': 100, 'data_deleteoflist': 101, 'music_playDrumForBeats': 102, 'music_menu_DRUM': 103, 'operator_mathop': 104, 'music_restForBeats': 105, 'sensing_resettimer': 106, 'operator_join': 107, 'event_broadcastandwait': 108, 'sensing_timer': 109, 'sensing_mousex': 110, 'data_replaceitemoflist': 111, 'motion_pointtowards': 112, 'motion_pointtowards_menu': 113, 'motion_setrotationstyle': 114, 'looks_cleargraphiceffects': 115, 'data_hidelist': 116, 'looks_size': 117, 'sensing_username': 118, 'sensing_mousey': 119, 'operator_mod': 120, 'operator_letter_of': 121, 'operator_length': 122, 'looks_nextbackdrop': 123, 'pen_penUp': 124, 'data_insertatlist': 125, 'data_showlist': 126, 'pen_penDown': 127, 'music_setInstrument': 128, 'music_menu_INSTRUMENT': 129, 'argument_reporter_boolean': 130, 'pen_setPenColorToColor': 131, 'pen_stamp': 132, 'operator_round': 133, 'event_whenstageclicked': 134, 'pen_setPenSizeTo': 135, 'event_whengreaterthan': 136, 'data_lengthoflist': 137, 'pen_changePenSizeBy': 138, 'music_setTempo': 139, 'pen_setPenHueToNumber': 140, 'pen_changePenHueBy': 141}\n"
     ]
    }
   ],
   "source": [
    "scripts_universal = scripts_train1 + scripts_train2 + scripts_train3\n",
    "\n",
    "\n",
    "# ------------- Universal scripts ----------------------------\n",
    "uni_block2idx, uni_idx2block = build_vocab(scripts_universal)\n",
    "uni_vocab_size = len(uni_block2idx)\n",
    "\n",
    "# ------------- Local context --------------------------------\n",
    "src_block2idx, src_idx2block = build_vocab(scripts_train1)\n",
    "#src_vocab_size = len(src_block2idx)\n",
    "src_vocab_size = uni_vocab_size\n",
    "trg_block2idx, trg_idx2block = build_vocab(scripts_train2)\n",
    "#trg_vocab_size = len(trg_block2idx)\n",
    "trg_vocab_size = uni_vocab_size\n",
    "neg_block2idx, neg_idx2block = build_vocab(scripts_train3)\n",
    "#neg_vocab_size = len(neg_block2idx)\n",
    "neg_vocab_size = uni_vocab_size\n",
    "print(trg_vocab_size)\n",
    "print(neg_vocab_size)\n",
    "print(uni_block2idx)\n",
    "# ------------ Global context -----------------------------\n",
    "src_block2idx_global, src_idx2block_global = build_vocab(scripts_train1_global)\n",
    "src_vocab_size_global = len(src_block2idx_global)\n",
    "trg_block2idx_global, trg_idx2block_global = build_vocab(scripts_train2_global)\n",
    "trg_vocab_size_global = len(trg_block2idx_global)\n",
    "neg_block2idx_global, neg_idx2block_global = build_vocab(scripts_train3_global)\n",
    "neg_vocab_size_global = len(neg_block2idx_global)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c195e6cc-511b-4a11-94bf-457d4dcaf9fc",
   "metadata": {},
   "source": [
    "### CREATION OF DATASET FOR **TEST**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da06562-d5cb-44e1-843b-4a82f09c3f5b",
   "metadata": {},
   "source": [
    "## TRAIN FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "83ad03ab-36f7-4d4d-a904-20d9b79b32d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn_old(batch):\n",
    "    trg_batch, src_batch, neg_batch = zip(*batch)\n",
    "    trg_batch = [seq[:MAX_SEQ_LEN].clone().detach() for seq in trg_batch]\n",
    "    src_batch = [seq[:MAX_SEQ_LEN].clone().detach() for seq in src_batch]\n",
    "    neg_batch = [seq[:MAX_SEQ_LEN].clone().detach() for seq in neg_batch]\n",
    "    trg_batch = torch.nn.utils.rnn.pad_sequence(trg_batch, batch_first=True, padding_value=0)\n",
    "    src_batch = torch.nn.utils.rnn.pad_sequence(src_batch, batch_first=True, padding_value=0)\n",
    "    neg_batch = torch.nn.utils.rnn.pad_sequence(neg_batch, batch_first=True, padding_value=0)\n",
    "    return src_batch, trg_batch, neg_batch\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    src_batch, pos_batch, neg_batch = zip(*batch)\n",
    "    src_batch = [seq.clone().detach() for seq in src_batch]\n",
    "    pos_batch = [seq.clone().detach() for seq in pos_batch]\n",
    "    neg_batch = [seq.clone().detach() for seq in neg_batch]\n",
    "    #print(trg_batch)\n",
    "    # Hacemos el padding sin truncar primero\n",
    "    #trg_batch = torch.nn.utils.rnn.pad_sequence(trg_batch, batch_first=True, padding_value=0)\n",
    "    #src_batch = torch.nn.utils.rnn.pad_sequence(src_batch, batch_first=True, padding_value=0)\n",
    "    #neg_batch = torch.nn.utils.rnn.pad_sequence(neg_batch, batch_first=True, padding_value=0)\n",
    "    \n",
    "    # Ahora truncamos las secuencias después del padding a MAX_SEQ_LEN\n",
    "    #trg_batch = trg_batch[:, :MAX_SEQ_LEN]\n",
    "    #src_batch = src_batch[:, :MAX_SEQ_LEN]\n",
    "    #neg_batch = neg_batch[:, :MAX_SEQ_LEN]\n",
    "\n",
    "    src_batch = [torch.nn.functional.pad(seq[:MAX_SEQ_LEN], (0, MAX_SEQ_LEN - len(seq[:MAX_SEQ_LEN])), value=0) for seq in src_batch]\n",
    "    pos_batch = [torch.nn.functional.pad(seq[:MAX_SEQ_LEN], (0, MAX_SEQ_LEN - len(seq[:MAX_SEQ_LEN])), value=0) for seq in pos_batch]\n",
    "    neg_batch = [torch.nn.functional.pad(seq[:MAX_SEQ_LEN], (0, MAX_SEQ_LEN - len(seq[:MAX_SEQ_LEN])), value=0) for seq in neg_batch]\n",
    "\n",
    "    src_batch = torch.stack(src_batch)    \n",
    "    pos_batch = torch.stack(pos_batch)\n",
    "    neg_batch = torch.stack(neg_batch)\n",
    "    return src_batch, pos_batch, neg_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "47bf392a-3758-4419-8d80-eeab901b8e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, loss_function, optimiser, epochs):\n",
    "    model.train()\n",
    "    final_anchor_embeddings = []\n",
    "    final_positive_embeddings = []\n",
    "    final_negative_embeddings = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for i, (anchor_batch, positive_batch, negative_batch) in enumerate(dataloader):\n",
    "            assert torch.max(anchor_batch) < src_vocab_size, f\"Anchor out of bounds: {torch.max(anchor_batch)} >= {src_vocab_size}\"\n",
    "            assert torch.max(positive_batch) < trg_vocab_size, f\"Positive out of bounds: {torch.max(positive_batch)} >= {trg_vocab_size}\"\n",
    "            assert torch.max(negative_batch) < trg_vocab_size, f\"Negative out of bounds: {torch.max(negative_batch)} >= {trg_vocab_size}\"\n",
    "            \n",
    "            anchor_batch = anchor_batch.to(device)\n",
    "            positive_batch = positive_batch.to(device)\n",
    "            negative_batch = negative_batch.to(device)\n",
    "            \n",
    "            # Zero grads\n",
    "            optimiser.zero_grad()\n",
    "\n",
    "            # Forward para anchor, positive y negative\n",
    "            anchor_embeddings = model(anchor_batch)\n",
    "            positive_embeddings = model(positive_batch)\n",
    "            negative_embeddings = model(negative_batch)\n",
    "            \n",
    "            # Almacenar los embeddings solo en el último epoch\n",
    "            if epoch == epochs - 1:\n",
    "                final_anchor_embeddings.append(anchor_embeddings.cpu().detach())\n",
    "                final_positive_embeddings.append(positive_embeddings.cpu().detach())\n",
    "                final_negative_embeddings.append(negative_embeddings.cpu().detach())\n",
    "\n",
    "\n",
    "            \n",
    "\n",
    "            # Calcular la pérdida de Triplet\n",
    "            loss = loss_function(anchor_embeddings, positive_embeddings, negative_embeddings)\n",
    "\n",
    "            # Backpropagation y actualización de parámetros\n",
    "            loss.backward()\n",
    "            optimiser.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        print(f'Epoch: {epoch + 1}/{epochs}, Loss: {avg_loss:.4f}')\n",
    "    \n",
    "    # Concatenar los embeddings del último epoch\n",
    "    final_anchor_embeddings = torch.cat(final_anchor_embeddings, dim=0)\n",
    "    final_positive_embeddings = torch.cat(final_positive_embeddings, dim=0)\n",
    "    final_negative_embeddings = torch.cat(final_negative_embeddings, dim=0)\n",
    "\n",
    "    return final_anchor_embeddings, final_positive_embeddings, final_negative_embeddings\n",
    "\n",
    "\n",
    "\n",
    "def evaluate(model, dataloader):\n",
    "    model.eval()  # Configura el modelo en modo de evaluación\n",
    "    with torch.no_grad():\n",
    "        all_anchor_embeddings = []\n",
    "        all_positive_embeddings = []\n",
    "        all_negative_embeddings = []\n",
    "        \n",
    "        # Recopila los embeddings para todos los lotes\n",
    "        for anchor_batch, positive_batch, negative_batch in dataloader:\n",
    "            anchor_batch = anchor_batch.to(device)\n",
    "            positive_batch = positive_batch.to(device)\n",
    "            negative_batch = negative_batch.to(device)\n",
    "            \n",
    "            # Obtener embeddings\n",
    "            anchor_embeddings = model.forward(anchor_batch)\n",
    "            positive_embeddings = model.forward(positive_batch)\n",
    "            negative_embeddings = model.forward(negative_batch)\n",
    "            \n",
    "            # Añadir a las listas\n",
    "            all_anchor_embeddings.append(anchor_embeddings)\n",
    "            all_positive_embeddings.append(positive_embeddings)\n",
    "            all_negative_embeddings.append(negative_embeddings)\n",
    "        \n",
    "        # Concatenar todos los embeddings\n",
    "        all_anchor_embeddings = torch.cat(all_anchor_embeddings, dim=0)\n",
    "        all_positive_embeddings = torch.cat(all_positive_embeddings, dim=0)\n",
    "        all_negative_embeddings = torch.cat(all_negative_embeddings, dim=0)\n",
    "    return anchor_embeddings, positive_embeddings, negative_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7234acb9-f0d4-4410-ba99-1b6ba4a7439b",
   "metadata": {},
   "source": [
    "## EXEC TRAIN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe0f17b-30e3-4499-9b0c-efdc345e9773",
   "metadata": {},
   "source": [
    "### Local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6834727c-a378-4682-9f70-32e3f94c306a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/5, Loss: 0.8812\n",
      "Epoch: 2/5, Loss: 0.6715\n",
      "Epoch: 3/5, Loss: 0.5881\n",
      "Epoch: 4/5, Loss: 0.5457\n",
      "Epoch: 5/5, Loss: 0.5094\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 16\n",
    "dataset_local = TripletDataset(scripts_train1, scripts_train2, scripts_train3, uni_block2idx)\n",
    "dataloader_local = DataLoader(dataset_local, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
    "model = Transformer(d_model=512, num_heads=8, d_ff=2048, num_layers=6,\n",
    "                    input_vocab_size=uni_vocab_size,\n",
    "                    max_len=MAX_SEQ_LEN, dropout=0.1)\n",
    "model = model.to(device)\n",
    "loss_function = triplet_loss = nn.TripletMarginLoss(margin=1.0)\n",
    "optimiser = optim.Adam(model.parameters(), lr=0.0001)\n",
    "# train local context\n",
    "anchor_embeddings_local, positive_embeddings_local, negative_embeddings_local = train(model, dataloader_local, loss_function, optimiser, epochs = 5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "beb20087-2063-439e-8f85-40e987da1f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save embeddings\n",
    "model._all_embeddings = anchor_embeddings_local\n",
    "\n",
    "# Save FULL model trained (arch and weights)\n",
    "#torch.save(model, 'action_local_scratch_triplet.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7fcdf95-7e3c-4c4c-a1db-05d4d28ca522",
   "metadata": {},
   "source": [
    "### Global"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e981b580-39f7-447b-a735-a51dab643bb0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/50, Loss: 2.3477\n",
      "Epoch: 2/50, Loss: 2.3471\n",
      "Epoch: 3/50, Loss: 2.3195\n",
      "Epoch: 4/50, Loss: 2.3222\n",
      "Epoch: 5/50, Loss: 2.3318\n",
      "Epoch: 6/50, Loss: 2.3139\n",
      "Epoch: 7/50, Loss: 2.3333\n",
      "Epoch: 8/50, Loss: 2.3268\n",
      "Epoch: 9/50, Loss: 2.3290\n",
      "Epoch: 10/50, Loss: 2.3317\n",
      "Epoch: 11/50, Loss: 2.3421\n",
      "Epoch: 12/50, Loss: 2.3593\n",
      "Epoch: 13/50, Loss: 2.3235\n",
      "Epoch: 14/50, Loss: 2.3136\n",
      "Epoch: 15/50, Loss: 2.3455\n",
      "Epoch: 16/50, Loss: 2.3241\n",
      "Epoch: 17/50, Loss: 2.3036\n",
      "Epoch: 18/50, Loss: 2.3496\n",
      "Epoch: 19/50, Loss: 2.3276\n",
      "Epoch: 20/50, Loss: 2.3173\n",
      "Epoch: 21/50, Loss: 2.3574\n",
      "Epoch: 22/50, Loss: 2.3418\n",
      "Epoch: 23/50, Loss: 2.2955\n",
      "Epoch: 24/50, Loss: 2.3250\n",
      "Epoch: 25/50, Loss: 2.3294\n",
      "Epoch: 26/50, Loss: 2.3201\n",
      "Epoch: 27/50, Loss: 2.3065\n",
      "Epoch: 28/50, Loss: 2.3050\n",
      "Epoch: 29/50, Loss: 2.3491\n",
      "Epoch: 30/50, Loss: 2.3426\n",
      "Epoch: 31/50, Loss: 2.3462\n",
      "Epoch: 32/50, Loss: 2.3009\n",
      "Epoch: 33/50, Loss: 2.3256\n",
      "Epoch: 34/50, Loss: 2.3211\n",
      "Epoch: 35/50, Loss: 2.3370\n",
      "Epoch: 36/50, Loss: 2.3190\n",
      "Epoch: 37/50, Loss: 2.3282\n",
      "Epoch: 38/50, Loss: 2.3364\n",
      "Epoch: 39/50, Loss: 2.3000\n",
      "Epoch: 40/50, Loss: 2.3003\n",
      "Epoch: 41/50, Loss: 2.3300\n",
      "Epoch: 42/50, Loss: 2.3192\n",
      "Epoch: 43/50, Loss: 2.3339\n",
      "Epoch: 44/50, Loss: 2.3526\n",
      "Epoch: 45/50, Loss: 2.3334\n",
      "Epoch: 46/50, Loss: 2.3405\n",
      "Epoch: 47/50, Loss: 2.3085\n",
      "Epoch: 48/50, Loss: 2.3164\n",
      "Epoch: 49/50, Loss: 2.3161\n",
      "Epoch: 50/50, Loss: 2.3341\n"
     ]
    }
   ],
   "source": [
    "\n",
    "BATCH_SIZE = 16\n",
    "dataset_global = TripletDataset(scripts_train1_global, scripts_train2_global, scripts_train3_global, uni_block2idx)\n",
    "dataloader_global = DataLoader(dataset_global, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
    "model_global = Transformer(d_model=512, num_heads=8, d_ff=2048, num_layers=6,\n",
    "                    input_vocab_size=uni_vocab_size,\n",
    "                    max_len=MAX_SEQ_LEN, dropout=0.1)\n",
    "model_global = model_global.to(device)\n",
    "loss_function_global = triplet_loss = nn.TripletMarginLoss(margin=1.0)\n",
    "optimiser_global = optim.Adam(model.parameters(), lr=0.0001)\n",
    "# train global context\n",
    "anchor_embeddings_global, positive_embeddings_global, negative_embeddings_global = train(model_global, dataloader_global, loss_function_global, optimiser_global, epochs = 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2e4c8de2-de60-4de4-9ceb-11ca83ef36b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save embeddings\n",
    "model_global._all_embeddings = anchor_embeddings_global\n",
    "\n",
    "# Save FULL model trained (arch and weights)\n",
    "torch.save(model_global, 'action_global_sprite_scratch_triplet.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ef2fb5-1715-474e-b533-7e5c481815fd",
   "metadata": {},
   "source": [
    "# GRAPH EMBEDDINGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe1d694-ad0b-4732-b1b5-f3a08745d887",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concat local and global embeddings\n",
    "\n",
    "anchor_embeddings = torch.cat((anchor_embeddings_local, anchor_embeddings_global), dim=-1)\n",
    "positive_embeddings = torch.cat((positive_embeddings_local, positive_embeddings_global), dim=-1)\n",
    "negative_embeddings = torch.cat((negative_embeddings_local, negative_embeddings_global), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cc390b8e-259b-4d71-aecb-0b6ac4cc7e43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061f0d75-a1d7-4c20-a633-12c22066bf00",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
